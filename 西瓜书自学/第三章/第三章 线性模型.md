## 3.1 基本形式：
  给定由d个属性描述的样本$x=(x_1;x_2;\cdots;x_d)$其中$x_i$是x在第i个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数：
$$f(x)=\omega_1x_1+\omega_2x_2+\cdots+\omega_dx_d+b$$
用向量形式：
$$f(x)=\omega^Tx+b$$
<font color=#FF44FF>线性模型有很好的可解释性(comprehensibility)</font>

## 3.2线性回归：
给定数据集$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)\}$
其中$x_i=(x_{i1};x_{i2};\cdots;x_{id}),y_i\in \Bbb R $

### **一元线性回归**：
考虑最简单情形：输入属性数目只有一个
即$D=\{(x_i,y_i)\}_{i=1}^m$
对于离散属性，若属性之间存在序关系，可通过连续化转化为连续值
*例如身高取值高矮{1.0,0.0}*
*三值属性高中低{1.0,0.5,0.0}*
若属性不存在序关系，假定k个属性，通常转为k维向量，例如瓜类，西瓜南瓜黄瓜(0,0,1),(0,1,0),(1,0,0)
<font color=#28FF28>若将无序属性连续化，会不恰当引入序关系，对后续处理造成误导</font>

学得目标：$f(x_i)=\omega x_i+b$

2.3节介绍过：均方误差是回归任务中最常用的性能度量，试图让均方误差最小化：
$$\left(w^*, b^*\right)=\underset{(w, b)}{\arg \min } \sum_{i=1}^m\left(f\left(x_i\right)-y_i\right)^2 \\
=\underset{(w, b)}{\arg \min } \sum_{i=1}^m\left(y_i-w x_i-b\right)^2$$
[arg含义](https://blog.csdn.net/joy_91/article/details/31346753)

均方误差几何意义：欧氏距离(Euclidean distance)，基于均方误差最小化来进行模型求解的方法称为最小二乘法(least square method)在线性回归中，最小二乘法就是找到一条直线，使所有样本到直线的欧氏距离最小
求解$\omega$和b使$E_(\omega,b)=\sum_{i=1}^m(y_i-\omega x_i-b)^2$最小化过程，称为线性回归模型最小二乘参数估计(parameter estimation)
将$E_(\omega,b)$对$\omega,b$分别求导得到：
$$
\begin{aligned}
&\frac{\partial E_{(w, b)}}{\partial w}=2\left(w \sum_{i=1}^m x_i^2-\sum_{i=1}^m\left(y_i-b\right) x_i\right), \\
&\frac{\partial E_{(w, b)}}{\partial b}=2\left(m b-\sum_{i=1}^m\left(y_i-w x_i\right)\right)
\end{aligned}
$$
令式(3.5)和 (3.6) 为零可得到 $w$ 和 $b$ 最优解的<font color=red>闭式(closed-form) 解</font>
$$
w=\frac{\sum_{i=1}^m y_i\left(x_i-\bar{x}\right)}{\sum_{i=1}^m x_i^2-\frac{1}{m}\left(\sum_{i=1}^m x_i\right)^2}\\b=\frac{1}{m}\sum_{i=1}^m(y_i-\omega x_i)
$$


### **多元线性回归**
**(multivariate linear regression)**
一般情形：数据集D，样本由d个属性描述$f(x_i)=\omega ^Tx_i+b$
把$\omega$和b吸收入向量形式$\hat \omega=(\omega;b)
数据集D表示为一个$m\times(d+1)大小的矩阵X,前d个元素对应示例的d个属性值，最后一个元素置为1$
$$
\mathbf{X}=\left(\begin{array}{ccccc}
x_{11} & x_{12} & \ldots & x_{1 d} & 1 \\
x_{21} & x_{22} & \ldots & x_{2 d} & 1 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
x_{m 1} & x_m 2 & \ldots & x_{m d} & 1
\end{array}\right)=\left(\begin{array}{cc}
x_1^{\mathrm{T}} & 1 \\
x_2^{\mathrm{T}} & 1 \\
\vdots & \vdots \\
x_m^{\mathrm{T}} & 1
\end{array}\right)
$
再把标记也写成向量形式 $y=\left(y_1 ; y_2 ; \ldots ; y_m\right)$, 则均方误差最小化有
$$
\hat{\boldsymbol{w}}^*=\underset{\hat{\boldsymbol{w}}}{\arg \min }(\boldsymbol{y}-\mathrm{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}}) .
$$
令 $E_{\hat{w}}=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{w}})$, 对 $\hat{\boldsymbol{w}}$ 求导得到
$$
\frac{\partial E_{\tilde{w}}}{\partial \hat{\boldsymbol{w}}}=2 \mathbf{X}^{\mathrm{T}}(\mathbf{X} \hat{\boldsymbol{w}}-\boldsymbol{y}) .
$$
令上式为零可得 $\hat{w}$ 最优解的闭式解, 但由于涉及矩阵逆的计算, 比单变量要复杂一些. 下面我们做一个简单的讨论.
当 $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ 为满秩矩阵(full-rank matrix)或正定矩阵(positive defin trix)时, 令式(3.10)为零可得
$$
\hat{\boldsymbol{w}}^*=\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y},
$$
令$\hat x_i=(x_i;1)$则最终学得的线性回归模型为$$f(\hat x_i)=\hat{x_i}^T\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}$$

现实中$X^TX$往往不是满秩，可以解出多个$\hat \omega$，选择哪一个由算法偏好决定，常见做法是引入正则项(regularization)

### **广义线性模型**：
**(generalized linear model)**
单调可微函数$g(\cdot)$
$y=g^{-1}(\omega^Tx+b)$

## 3.：3对数几率回归：
二分类任务，输出标记{0,1}，找一个单调可微函数代替单位阶跃函数：
<font color=green>对数几率函数</font>
$$y=\frac{1}{1+e^{-z}}$$
$$
y=\frac{1}{1+e^{-\left(w^{\mathrm{T}} \boldsymbol{x}+b\right)}} .
$$
可变化为
$$
\ln \frac{y}{1-y}=w^{\mathrm{T}} \boldsymbol{x}+b .
$$
若将 $y$ 视为样本 $x$ 作为正例的可能性, 则 $1-y$ 是其反例可能性, 两者的比值
$$
\frac{y}{1-y}
$$
称为 “几率” (odds), __反映了 $x$ 作为正例的相对可能性__ 对几率取对数则得 "对数几率" (log odds, 亦称 logit)
$$
\ln \frac{y}{1-y} .
$$

$$
\ln \frac{p(y=1 \mid \boldsymbol{x})}{p(y=0 \mid \boldsymbol{x})}=w^{\mathrm{T}} \boldsymbol{x}+b
$$
$$
\begin{aligned}
&p(y=1 \mid \boldsymbol{x})=\frac{e^{w^{\mathrm{T}} \boldsymbol{x}+b}}{1+e^{w^{\mathrm{T}} \boldsymbol{x}+b}}, \\
&p(y=0 \mid \boldsymbol{x})=\frac{1}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}} .
\end{aligned}
$$
对率回归模型最大化 “对数似然” 
$$
\ell(\boldsymbol{w}, b)=\sum_{i=1}^m \ln p\left(y_i \mid \boldsymbol{x}_i ; \boldsymbol{w}, b\right),
$$
令$\beta=(\omega;b),\hat x=(x;1)$则$\omega^Tx+b$可简写为$\beta^T\hat x$
等价于最小化 $$
\ell(\boldsymbol{\beta})=\sum_{i=1}^m\left(-y_i \boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}_i+\ln \left(1+e^{\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}_i}\right)\right)
$$
 经典的数值优化算法如梯度下降法(gradient descent nethod)、牛顿法(Newton method)等都可求得其最优解, 于是就得到 $\beta^*=arg min\ell(\beta)
$
[参考文章](https://blog.csdn.net/jiang1350/article/details/125635204)

## 3.4线性判别分析：
简称LDA
* 思想：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类尽可能远离